#!/usr/bin/env python3
"""
üåå Enhanced FSOT Space Telescope Science Institute Archive Explorer
================================================================

Advanced autonomous web exploration of the STScI archive system with enhanced
debugging and dynamic content handling for complex scientific platforms.

Author: FSOT Neuromorphic AI System
Date: September 5, 2025
Target: https://archive.stsci.edu/
"""

import sys
import os
import json
import time
from datetime import datetime

# Add the current directory to Python path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from fsot_autonomous_web_crawler import FSotAutonomousWebCrawler
    print("üåå Enhanced FSOT Space Telescope Archive Explorer Initialized!")
except ImportError as e:
    print(f"‚ùå Import Error: {e}")
    print("üîß Please ensure fsot_autonomous_web_crawler.py is in the current directory")
    sys.exit(1)

def analyze_stsci_archive_enhanced():
    """
    üî≠ Enhanced comprehensive analysis of the Space Telescope Science Institute Archive
    
    This enhanced version includes:
    - Extended loading times for complex scientific platforms
    - Enhanced debugging and content detection
    - Alternative content extraction methods
    - Robust error handling for dynamic sites
    """
    
    print("\n" + "="*80)
    print("üåå ENHANCED FSOT SPACE TELESCOPE ARCHIVE EXPLORATION")
    print("="*80)
    print("üéØ Target: https://archive.stsci.edu/")
    print("üöÄ Mission: Enhanced Scientific Platform Analysis")
    print("üß† AI System: FSOT Neuromorphic Intelligence")
    print("üî¨ Enhancement: Dynamic Content & Extended Analysis")
    print("="*80 + "\n")
    
    # Initialize the autonomous web crawler
    crawler = None
    analysis_results = {
        'exploration_timestamp': datetime.now().strftime("%Y%m%d_%H%M%S"),
        'target_url': 'https://archive.stsci.edu/',
        'platform_type': 'Scientific Data Archive',
        'mission_status': 'INITIATED',
        'browser_diagnostics': {},
        'content_extraction_methods': [],
        'page_source_analysis': {},
        'navigation_attempts': [],
        'scientific_intelligence': {},
        'debug_information': []
    }
    
    try:
        print("üîß Initializing Enhanced FSOT Autonomous Web Crawler...")
        crawler = FSotAutonomousWebCrawler()
        print("‚úÖ Crawler initialized successfully!")
        
        print("üåê Setting up enhanced browser environment...")
        browser_ready = crawler.initialize_browser(headless=False)
        if not browser_ready:
            print("‚ùå Failed to initialize browser")
            analysis_results['mission_status'] = 'BROWSER_INIT_FAILED'
            return analysis_results
        print("‚úÖ Enhanced browser initialized successfully!")
        
        print("\nüåê Navigating to STScI Archive...")
        success = crawler.navigate_to_url('https://archive.stsci.edu/')
        
        if not success:
            print("‚ùå Failed to navigate to STScI Archive")
            analysis_results['mission_status'] = 'NAVIGATION_FAILED'
            return analysis_results
        
        print("‚úÖ Successfully navigated to STScI Archive!")
        
        # Enhanced loading sequence for scientific platforms
        print("‚è≥ Enhanced loading sequence for dynamic scientific content...")
        time.sleep(8)  # Extended wait for complex scientific platforms
        
        # Check page readiness
        try:
            if crawler.driver:
                page_ready = crawler.driver.execute_script("return document.readyState")
                print(f"üìä Page readiness state: {page_ready}")
                analysis_results['browser_diagnostics']['page_ready_state'] = page_ready
            else:
                print("‚ö†Ô∏è Browser driver not available for readiness check")
        except Exception as e:
            print(f"‚ö†Ô∏è Could not check page readiness: {e}")
            analysis_results['debug_information'].append(f"Page readiness check failed: {e}")
        
        # Get basic page information
        try:
            if crawler.driver:
                current_url = crawler.driver.current_url
                page_title = crawler.driver.title
                print(f"üåê Current URL: {current_url}")
                print(f"üìÑ Page Title: {page_title}")
                analysis_results['browser_diagnostics'].update({
                    'final_url': current_url,
                    'page_title': page_title
                })
            else:
                print("‚ö†Ô∏è Browser driver not available for page info")
        except Exception as e:
            print(f"‚ö†Ô∏è Could not get basic page info: {e}")
            analysis_results['debug_information'].append(f"Basic page info failed: {e}")
        
        # Method 1: Use crawler's intelligent exploration
        print("\nüîç Method 1: Intelligent Platform Exploration...")
        try:
            exploration_data = crawler.explore_page_intelligently()
            if exploration_data:
                print("‚úÖ Intelligent exploration successful!")
                analysis_results['content_extraction_methods'].append('intelligent_exploration')
                analysis_results['intelligent_exploration_data'] = exploration_data
            else:
                print("‚ö†Ô∏è Intelligent exploration returned no data")
                analysis_results['debug_information'].append("Intelligent exploration returned None")
        except Exception as e:
            print(f"‚ùå Intelligent exploration error: {e}")
            analysis_results['debug_information'].append(f"Intelligent exploration error: {e}")
        
        # Method 2: Direct page source analysis
        print("\nüîç Method 2: Direct Page Source Analysis...")
        try:
            if crawler.driver:
                page_source = crawler.driver.page_source
                source_length = len(page_source)
                print(f"üìÑ Page source length: {source_length} characters")
                
                if source_length > 0:
                    analysis_results['content_extraction_methods'].append('page_source')
                    analysis_results['page_source_analysis'] = {
                        'source_length': source_length,
                        'contains_content': source_length > 1000
                    }
                    
                    # Check for scientific keywords in page source
                    scientific_terms = [
                        'hubble', 'telescope', 'jwst', 'kepler', 'spitzer', 'galex',
                        'archive', 'data', 'observation', 'spectrum', 'image',
                        'mission', 'instrument', 'detector', 'stsci', 'nasa',
                        'astronomical', 'cosmic', 'galaxy', 'star', 'research'
                    ]
                    
                    source_lower = page_source.lower()
                    found_terms = [term for term in scientific_terms if term in source_lower]
                    
                    print(f"üî¨ Scientific terms found: {len(found_terms)}")
                    if found_terms:
                        print(f"   Keywords: {', '.join(found_terms[:10])}")  # Show first 10
                    
                    analysis_results['scientific_intelligence'] = {
                        'scientific_terms_found': found_terms,
                        'scientific_density': len(found_terms),
                        'platform_detected': len(found_terms) > 0
                    }
                    
                    # Check for specific STScI content
                    stsci_indicators = ['stsci', 'space telescope', 'hubble', 'jwst', 'archive']
                    stsci_found = [term for term in stsci_indicators if term in source_lower]
                    
                    if stsci_found:
                        print(f"üéØ STScI platform confirmed! Found: {', '.join(stsci_found)}")
                        analysis_results['scientific_intelligence']['stsci_confirmed'] = True
                        analysis_results['scientific_intelligence']['stsci_indicators'] = stsci_found
                    else:
                        print("‚ö†Ô∏è STScI indicators not detected in page source")
                    
                else:
                    print("‚ùå Page source is empty")
                    analysis_results['debug_information'].append("Page source is empty")
            else:
                print("‚ö†Ô∏è Browser driver not available for page source analysis")
                
        except Exception as e:
            print(f"‚ùå Page source analysis error: {e}")
            analysis_results['debug_information'].append(f"Page source analysis error: {e}")
        
        # Method 3: Element-by-element detection
        print("\nüîç Method 3: Element Detection...")
        try:
            if crawler.driver:
                from selenium.webdriver.common.by import By
                
                # Check for links
                links = crawler.driver.find_elements(By.TAG_NAME, "a")
                print(f"üîó Links found: {len(links)}")
                
                # Check for forms
                forms = crawler.driver.find_elements(By.TAG_NAME, "form")
                print(f"üìù Forms found: {len(forms)}")
                
                # Check for images
                images = crawler.driver.find_elements(By.TAG_NAME, "img")
                print(f"üñºÔ∏è Images found: {len(images)}")
                
                # Check for headings
                headings = crawler.driver.find_elements(By.CSS_SELECTOR, "h1, h2, h3, h4, h5, h6")
                print(f"üìã Headings found: {len(headings)}")
                
                analysis_results['content_extraction_methods'].append('element_detection')
                analysis_results['element_detection'] = {
                    'links_count': len(links),
                    'forms_count': len(forms),
                    'images_count': len(images),
                    'headings_count': len(headings),
                    'sample_link_texts': []  # Initialize as empty list
                }
                
                # Extract some link text for analysis
                if links:
                    link_texts = []
                    for link in links[:10]:  # First 10 links
                        try:
                            text = link.text.strip()
                            if text:
                                link_texts.append(text)
                        except:
                            pass
                    
                    if link_texts:
                        print(f"üîó Sample link texts: {', '.join(link_texts[:5])}")
                        analysis_results['element_detection']['sample_link_texts'] = link_texts
            else:
                print("‚ö†Ô∏è Browser driver not available for element detection")
            
        except Exception as e:
            print(f"‚ùå Element detection error: {e}")
            analysis_results['debug_information'].append(f"Element detection error: {e}")
        
        # Final assessment
        methods_used = len(analysis_results['content_extraction_methods'])
        print(f"\nüìä Content extraction methods used: {methods_used}")
        
        if methods_used > 0:
            analysis_results['mission_status'] = 'ENHANCED_EXPLORATION_COMPLETE'
            print("‚úÖ Enhanced STScI Archive exploration completed successfully!")
        else:
            analysis_results['mission_status'] = 'CONTENT_EXTRACTION_FAILED'
            print("‚ö†Ô∏è All content extraction methods failed")
        
    except Exception as e:
        print(f"‚ùå Critical error during enhanced STScI exploration: {str(e)}")
        analysis_results['mission_status'] = 'CRITICAL_ERROR'
        analysis_results['critical_error'] = str(e)
        
    finally:
        # Clean up browser resources
        if crawler:
            try:
                print("\nüßπ Cleaning up browser resources...")
                crawler.cleanup()
                print("‚úÖ Browser cleanup completed")
            except Exception as cleanup_error:
                print(f"‚ö†Ô∏è Cleanup warning: {cleanup_error}")
    
    # Save enhanced analysis report
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_filename = f"STScI_Enhanced_Analysis_{timestamp}.json"
    
    try:
        with open(report_filename, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, indent=2, ensure_ascii=False)
        print(f"üìÑ Enhanced analysis report saved: {report_filename}")
    except Exception as save_error:
        print(f"‚ö†Ô∏è Failed to save report: {save_error}")
    
    return analysis_results

def main():
    """
    üåå Main execution function for enhanced STScI Archive exploration
    """
    print("üöÄ Enhanced FSOT Space Telescope Science Institute Archive Explorer")
    print("üéØ Demonstrating advanced scientific platform analysis with enhanced debugging")
    
    # Execute the enhanced analysis
    results = analyze_stsci_archive_enhanced()
    
    # Display enhanced summary
    print("\n" + "="*80)
    print("üèÜ ENHANCED STSCI ARCHIVE EXPLORATION SUMMARY")
    print("="*80)
    print(f"üìä Mission Status: {results.get('mission_status', 'UNKNOWN')}")
    print(f"üî¨ Content Extraction Methods: {len(results.get('content_extraction_methods', []))}")
    print(f"üß† Debug Information Entries: {len(results.get('debug_information', []))}")
    
    # Show browser diagnostics
    if results.get('browser_diagnostics'):
        diag = results['browser_diagnostics']
        print(f"\nüåê Browser Diagnostics:")
        print(f"   üìÑ Page Title: {diag.get('page_title', 'Unknown')}")
        print(f"   üåê Final URL: {diag.get('final_url', 'Unknown')}")
        print(f"   ‚úÖ Page Ready: {diag.get('page_ready_state', 'Unknown')}")
    
    # Show scientific intelligence
    if results.get('scientific_intelligence'):
        sci_intel = results['scientific_intelligence']
        print(f"\nüî¨ Scientific Intelligence:")
        print(f"   üåå Scientific Terms: {sci_intel.get('scientific_density', 0)}")
        print(f"   üéØ STScI Confirmed: {sci_intel.get('stsci_confirmed', False)}")
        if sci_intel.get('stsci_indicators'):
            print(f"   üîç STScI Indicators: {', '.join(sci_intel['stsci_indicators'][:5])}")
    
    # Show element detection results
    if results.get('element_detection'):
        elem = results['element_detection']
        print(f"\nüîç Element Detection:")
        print(f"   üîó Links: {elem.get('links_count', 0)}")
        print(f"   üìù Forms: {elem.get('forms_count', 0)}")
        print(f"   üñºÔ∏è Images: {elem.get('images_count', 0)}")
        print(f"   üìã Headings: {elem.get('headings_count', 0)}")
    
    print("="*80)
    print("üåå Enhanced FSOT STScI Archive exploration completed! üî≠‚ú®")

if __name__ == "__main__":
    main()
